{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b234ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "\n",
    "from collections import defaultdict as ddict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebdcbc9-6991-45b3-91ee-253c45f0e1c8",
   "metadata": {},
   "source": [
    "# Step 1: Download the original MASSIVE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "968338bf-1f1a-4952-b91d-f3295242f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASSIVE data: https://github.com/alexa/massive\n",
    "\n",
    "# !wget https://amazon-massive-nlu-dataset.s3.amazonaws.com/amazon-massive-dataset-1.1.tar.gz\n",
    "# !tar -xvzf amazon-massive-dataset-1.1.tar.gz\n",
    "\n",
    "# Update path to where you stored the MASSIVE dataset\n",
    "\n",
    "data_path = Path('../data/1.1/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f966296-cedd-4f98-ab7b-d038a41d2080",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "\n",
    "for path in data_path.iterdir():\n",
    "\tif '._' not in str(path):\n",
    "\t\twith open(path, 'r') as fin:\n",
    "\t\t\tfor line in fin:\n",
    "\t\t\t\tl = json.loads(line)\n",
    "\n",
    "\t\t\t\t# We work with the 1685 utterances in the QA split (ignoring other 18K examples in MASSIVE)\n",
    "\t\t\t\tif l['scenario']=='qa':\n",
    "\t\t\t\t\tif l['id'] not in data:\n",
    "\t\t\t\t\t\tdata[l['id']] = []\n",
    "\t\t\t\t\tdata[l['id']].append(l)\n",
    "\n",
    "write_data = False\n",
    "\n",
    "if write_data:\n",
    "\n",
    "    with open('qa_all_langs_dict.json', 'w') as fout:\n",
    "       json.dump(data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be04c8b7-8d5d-4227-875f-4064abfa9b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1685"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788609aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '681',\n",
       "  'locale': 'sv-SE',\n",
       "  'partition': 'train',\n",
       "  'scenario': 'qa',\n",
       "  'intent': 'qa_stock',\n",
       "  'utt': 'håll mig uppdaterad om börskurser',\n",
       "  'annot_utt': 'håll mig uppdaterad om [news_topic : börskurser]',\n",
       "  'worker_id': '3',\n",
       "  'slot_method': [{'slot': 'news_topic', 'method': 'translation'}],\n",
       "  'judgments': [{'worker_id': '2',\n",
       "    'intent_score': 1,\n",
       "    'slots_score': 1,\n",
       "    'grammar_score': 4,\n",
       "    'spelling_score': 2,\n",
       "    'language_identification': 'target'},\n",
       "   {'worker_id': '0',\n",
       "    'intent_score': 1,\n",
       "    'slots_score': 1,\n",
       "    'grammar_score': 4,\n",
       "    'spelling_score': 2,\n",
       "    'language_identification': 'target'},\n",
       "   {'worker_id': '20',\n",
       "    'intent_score': 1,\n",
       "    'slots_score': 1,\n",
       "    'grammar_score': 4,\n",
       "    'spelling_score': 2,\n",
       "    'language_identification': 'target'}]},\n",
       " {'id': '681',\n",
       "  'locale': 'sw-KE',\n",
       "  'partition': 'train',\n",
       "  'scenario': 'qa',\n",
       "  'intent': 'qa_stock',\n",
       "  'utt': 'nisasishe kuhusu bei za soko la hisa',\n",
       "  'annot_utt': 'nisasishe kuhusu [news_topic : bei za soko la hisa]',\n",
       "  'worker_id': '48',\n",
       "  'slot_method': [{'slot': 'news_topic', 'method': 'translation'}],\n",
       "  'judgments': [{'worker_id': '52',\n",
       "    'intent_score': 1,\n",
       "    'slots_score': 1,\n",
       "    'grammar_score': 4,\n",
       "    'spelling_score': 2,\n",
       "    'language_identification': 'target'},\n",
       "   {'worker_id': '60',\n",
       "    'intent_score': 1,\n",
       "    'slots_score': 1,\n",
       "    'grammar_score': 4,\n",
       "    'spelling_score': 2,\n",
       "    'language_identification': 'target'},\n",
       "   {'worker_id': '36',\n",
       "    'intent_score': 1,\n",
       "    'slots_score': 1,\n",
       "    'grammar_score': 4,\n",
       "    'spelling_score': 2,\n",
       "    'language_identification': 'target'}]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example data instance, show 2 (of 52) languages\n",
    "\n",
    "data['681'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a3e4923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'qa_stock': 202, 'qa_factoid': 775, 'qa_definition': 379, 'qa_maths': 116, 'qa_currency': 213})\n"
     ]
    }
   ],
   "source": [
    "# Get a look at the different types of questions in the QA split\n",
    "\n",
    "intent_count = ddict(int)\n",
    "\n",
    "for k, v in data.items():\n",
    "    eng_item = v[0]\n",
    "    intent_count[eng_item['intent']]+=1\n",
    "    \n",
    "print(intent_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27c8f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting sample from qa_factoid\n",
    "\n",
    "qa_factoid_sample = list()\n",
    "\n",
    "for k, v in data.items():\n",
    "    for item in v:\n",
    "        if item['locale']=='en-US':\n",
    "            if item['intent']=='qa_factoid':\n",
    "                if random.random() < 0.1:\n",
    "                    qa_factoid_sample.append((item['utt'], item['id']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a32b26b-a4a6-4e41-9650-767ea545e9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('when does the super bowl officially start', '9138')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_factoid_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff154934",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_data = False\n",
    "\n",
    "if write_data:\n",
    "\n",
    "    with open('qa_factoid_sample.csv', 'w') as fout:\n",
    "        csv_out=csv.writer(fout)\n",
    "        csv_out.writerow(['utt', 'id'])\n",
    "        for row in qa_factoid_sample:\n",
    "            csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed94a0-1940-4656-8d58-4bd71d5d71a5",
   "metadata": {},
   "source": [
    "# Step 2: Get the MASSIVE-AMR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11a1da74-989c-415c-a165-564e7e412bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/amazon-science/MASSIVE-AMR.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e0d3fd9-b0ba-44a7-97f6-133cd71ecc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is plain AMRs: data/massive_amr.txt\n",
    "# This file contains MASSIVE annotations for EN-only: data/massive_amr.jsonl\n",
    "\n",
    "# import jsonlines\n",
    "\n",
    "# Set path to the massive_amr.txt\n",
    "\n",
    "amr_path = '../data/massive_amr.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4918342-eb16-43ee-a203-2445312b7987",
   "metadata": {},
   "source": [
    "# Step 3: Process and validate AMRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c97607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " AMR Sentence Object\n",
    " From: <https://github.com/panx27/amr-reader>\n",
    "'''\n",
    "\n",
    "class Sentence(object):\n",
    "    def __init__(self, sentid='', sent='', raw_amr='', comments='',\n",
    "                 amr_nodes=dict(), graph=list()):\n",
    "        self.sentid = sentid         # Sentence id\n",
    "        self.sent = sent             # Sentence\n",
    "        self.raw_amr = raw_amr       # Raw AMR\n",
    "        self.comments = comments     # Comments\n",
    "        self.amr_nodes = amr_nodes   # AMR ndoes table\n",
    "        self.graph = graph           # Path of the whole graph\n",
    "        self.amr_paths = dict()      # AMR paths\n",
    "        self.named_entities = dict() # Named entities\n",
    "\n",
    "    def __str__(self):\n",
    "        return '%s%s\\n' % (self.comments, self.raw_amr)\n",
    "\n",
    "\n",
    "def amr_validator(raw_amr): # TODO: add more test cases\n",
    "    '''\n",
    "    AMR validator\n",
    "\n",
    "    :param str raw_amr:\n",
    "    :return bool:\n",
    "    '''\n",
    "    if raw_amr.count('(') == 0:\n",
    "        return False\n",
    "    if raw_amr.count(')') == 0:\n",
    "        return False\n",
    "    if raw_amr.count('(') != raw_amr.count(')'):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def main_process_amrs(raw_amrs):\n",
    "    '''\n",
    "    :param str raw_amrs: input raw amrs, separated by '\\n'\n",
    "    :return list res: Sentence objects\n",
    "    '''\n",
    "    res = []\n",
    "    for i in re.split('\\n\\s*\\n', raw_amrs):\n",
    "        sent = re.search('::snt (.*?)\\n', i)\n",
    "        sent = sent.group(1) if sent else ''\n",
    "        sentid = re.search('::id (.*?)\\n', i)\n",
    "        if sentid:\n",
    "            sentid = sentid.group(1)\n",
    "        else:\n",
    "            sentid = uuid.uuid4()\n",
    "\n",
    "        raw_amr = ''\n",
    "        comments = ''\n",
    "        for line in i.splitlines(True):\n",
    "            if line.startswith('# '):\n",
    "                comments += line\n",
    "                continue\n",
    "\n",
    "            # convert '( )' to '%28 %29' in :wiki\n",
    "            m = re.search(':wiki\\s\\\"(.+?)\\\"', line)\n",
    "            if m:\n",
    "                line = line.replace(m.group(1),\n",
    "                                    urllib.parse.quote_plus(m.group(1)))\n",
    "\n",
    "            # convert '( )' to '%28 %29' in :name\n",
    "            m = re.findall('\\\"(\\S+)\\\"', line)\n",
    "            for i in m:\n",
    "                if '(' in i or ')' in i:\n",
    "                    line = line.replace(i, urllib.parse.quote_plus(i))\n",
    "            raw_amr += line\n",
    "\n",
    "        if not raw_amr:\n",
    "            continue\n",
    "        if not amr_validator(raw_amr):\n",
    "            raise Exception('Invalid raw AMR: %s' % sentid)\n",
    "\n",
    "        sent_obj = Sentence(sentid, sent, raw_amr, comments)\n",
    "        res.append(sent_obj)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1cb84b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_amrs = list()\n",
    "\n",
    "with open(amr_path, 'r') as f:\n",
    "    raw_amrs = f.read()\n",
    "    res = main_process_amrs(raw_amrs)\n",
    "    list_amrs += res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aa4bde0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1685"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_amrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd795d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what are some updates about the stock market'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_amrs[0].sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74e12e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u / update-02\n",
      "      :ARG2 (a / amr-unknown)\n",
      "      :topic (m / market-01\n",
      "            :ARG1 (s / stock))\n",
      "      :mod (s2 / some))\n"
     ]
    }
   ],
   "source": [
    "print(list_amrs[0].raw_amr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df9e3e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2997'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_amrs[0].sentid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24aaac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the multilingual entities from the original MASSIVE data\n",
    "# TODO: Check how many annotations for entities actually exist (some languages not completed)\n",
    "\n",
    "def create_named_entity(name):\n",
    "\n",
    "    # Preprocessing the entity annotations\n",
    "    \n",
    "    named_entity = '/ name '\n",
    "    \n",
    "    name = name.split()\n",
    "    thisIdx = 1\n",
    "    for idx, n in enumerate(name):\n",
    "        if n not in ['the', 'this']:\n",
    "            n = n.replace(\"'s\", \"\")\n",
    "            token = f':op{thisIdx} \"{n.strip()}\" '\n",
    "            named_entity += token\n",
    "            thisIdx += 1\n",
    "      \n",
    "    named_entity = named_entity.strip()\n",
    "    named_entity += ')'\n",
    "     \n",
    "    return named_entity\n",
    "\n",
    "def get_name_from_annotation(utt):\n",
    "    \n",
    "    entities = []\n",
    "    \n",
    "    entity_found = re.findall('\\[(.*?)\\]', utt)\n",
    "    \n",
    "    if entity_found:\n",
    "\n",
    "        #entity = entity.group(1).split(':')[1]\n",
    "        #entity = create_named_entity(entity.strip()) \n",
    "        \n",
    "        for ef in entity_found:\n",
    "            ef_split = ef.split(':')[1]\n",
    "            ef_split = create_named_entity(ef_split.strip())\n",
    "            ef_type = ef.split(':')[0].strip()\n",
    "            entities.append((ef_type, ef_split))\n",
    "         \n",
    "    else:\n",
    "        entities = [(0,'NONE')]\n",
    "        \n",
    "    return entities\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d9541a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making text-to-AMR mappings for all languages\n",
    "# Collecting some statistics\n",
    "# Making training, validation, and test splits\n",
    "# Output: Standard txt files for AMR data, fields are: ::id, ::en_utt, ::annot_utt, ::snt (in L2) followed by AMR\n",
    "\n",
    "snt_lens_tokens, snt_lens_chars = list(), list()\n",
    "snt_lens_tokens_en, snt_lens_chars_en = list(), list()\n",
    "\n",
    "train_path = Path('../data/amrs-massive-train.txt')\n",
    "val_path = Path('../data/amrs-massive-val.txt')\n",
    "test_path = Path('../data/amrs-massive-test.txt')\n",
    "\n",
    "train, val, test = [], [], []\n",
    "\n",
    "for idx, item in enumerate(list_amrs):\n",
    "    sentid = item.sentid\n",
    "    raw_amr = item.raw_amr\n",
    "    \n",
    "    all_elements = data[sentid]\n",
    "    \n",
    "    for element in all_elements:\n",
    "        en_utt = ''\n",
    "        if element['locale'] == 'en-US':\n",
    "            en_utt = element['annot_utt']  \n",
    "            \n",
    "            snt_lens_tokens_en.append(len(en_utt.split()))\n",
    "            snt_lens_chars_en.append(len(en_utt))\n",
    "            break      \n",
    "            \n",
    "    en_entities = get_name_from_annotation(en_utt)\n",
    "    \n",
    "    for element in all_elements:\n",
    "        \n",
    "        utt = element['utt']\n",
    "        \n",
    "        snt_lens_tokens.append(len(utt.split()))\n",
    "        snt_lens_chars.append(len(utt))\n",
    "        \n",
    "        annot_utt = element['annot_utt']\n",
    "        sentid = element['id']\n",
    "        locale = element['locale']\n",
    "        \n",
    "        other_entities = get_name_from_annotation(annot_utt)\n",
    "        \n",
    "        temp_raw_amr = raw_amr\n",
    "        \n",
    "        if 'definition_word' in annot_utt:\n",
    "            # Questions about definitions do not appear to be localized, there might be other cases\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            en_entities = sorted(en_entities)\n",
    "            other_entities = sorted(other_entities)\n",
    "            for en_entity, ot_entity in zip(en_entities, other_entities):\n",
    "                \n",
    "                if en_entity[0]==ot_entity[0] and en_entity[1] != 'NONE' and ot_entity[1] != 'NONE':\n",
    "                    #print(en_entity[1], ot_entity[1])\n",
    "                    temp_raw_amr = temp_raw_amr.replace(en_entity[1], ot_entity[1])\n",
    "                    #temp_raw_amr = temp_temp_raw_amr\n",
    "\n",
    "\n",
    "        # Arbitrary choices! Decide on your own splits! \n",
    "        # Standard format for AMR txt files\n",
    "        if idx < 60:\n",
    "                val.append(f'# ::id {sentid}-{locale}\\n')\n",
    "                val.append('# ::en_utt '+ en_utt + '\\n')\n",
    "                val.append('# ::annot_utt '+ annot_utt + '\\n')\n",
    "                val.append('# ::snt '+ utt + '\\n')\n",
    "                val.append(temp_raw_amr)\n",
    "                val.append('\\n')\n",
    "                val.append('\\n')\n",
    "\n",
    "        elif idx < 120:\n",
    "                test.append(f'# ::id {sentid}-{locale}\\n')\n",
    "                test.append('# ::en_utt '+ en_utt + '\\n')\n",
    "                test.append('# ::annot_utt '+ annot_utt + '\\n')\n",
    "                test.append('# ::snt '+ utt + '\\n')\n",
    "                test.append(temp_raw_amr)\n",
    "                test.append('\\n')\n",
    "                test.append('\\n')\n",
    "\n",
    "        else:\n",
    "                train.append(f'# ::id {sentid}-{locale}\\n')\n",
    "                train.append('# ::en_utt '+ en_utt + '\\n')\n",
    "                train.append('# ::annot_utt '+ annot_utt + '\\n')\n",
    "                train.append('# ::snt '+ utt + '\\n')\n",
    "                train.append(temp_raw_amr)\n",
    "                train.append('\\n')\n",
    "                train.append('\\n')\n",
    "\n",
    "write_data = False\n",
    "\n",
    "if write_data:\n",
    "    \n",
    "    with open(train_path, 'w') as outfile:\n",
    "            outfile.writelines(train)\n",
    "\n",
    "    with open(val_path, 'w') as outfile:\n",
    "            outfile.writelines(val)\n",
    "\n",
    "    with open(test_path, 'w') as outfile:\n",
    "            outfile.writelines(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9003b1a2-71a8-4bbf-83da-f45fa4cb64f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# ::id 13965-ar-SA\n",
      "# ::en_utt what is the exchange rate between [currency_name : u. s. d.] and [currency_name : euro]\n",
      "# ::annot_utt كم سعر صرف بين [currency_name : الريال السعودي] و [currency_name : الدينار الأردني]\n",
      "# ::snt كم سعر صرف بين الريال السعودي و الدينار الأردني\n",
      "(r / rate-01\n",
      "      :ARG1 (e / exchange-01\n",
      "            :ARG1 (c / currency :name (n / name :op1 \"الريال\" :op2 \"السعودي\"))\n",
      "            :ARG3 (c2 / currency :name (n2 / name :op1 \"الدينار\" :op2 \"الأردني\")))\n",
      "      :ARG2 (a / amr-unknown))\n",
      "\n",
      "# ::id 13965-ka-GE\n",
      "# ::en_utt what is the exchange rate between [currency_name : u. s. d.] and [currency_name : euro]\n",
      "# ::annot_utt რა არის გაცვლითი კურსი [currency_name : ამერიკულ დოლარსა] და [currency_name : ევროს] შორის\n",
      "# ::snt რა არის გაცვლითი კურსი ამერიკულ დოლარსა და ევროს შორის\n",
      "(r / rate-01\n",
      "      :ARG1 (e / exchange-01\n",
      "            :ARG1 (c / currency :name (n / name :op1 \"ევროს\"))\n",
      "            :ARG3 (c2 / currency :name (n2 / name :op1 \"ამერიკულ\" :op2 \"დოლარსა\")))\n",
      "      :ARG2 (a / amr-unknown))\n",
      "\n",
      "# ::id 13965-te-IN\n",
      "# ::en_utt what is the exchange rate between [currency_name : u. s. d.] and [currency_name : euro]\n",
      "# ::annot_utt [currency_name : యునైటెడ్ స్టేట్స్ డాలర్] మరియు [currency_name : యూరో] మధ్య మారకం రేటు ఎంత\n",
      "# ::snt యునైటెడ్ స్టేట్స్ డాలర్ మరియు యూరో మధ్య మారకం రేటు ఎంత\n",
      "(r / rate-01\n",
      "      :ARG1 (e / exchange-01\n",
      "            :ARG1 (c / currency :name (n / name :op1 \"యూరో\"))\n",
      "            :ARG3 (c2 / currency :name (n2 / name :op1 \"యునైటెడ్\" :op2 \"స్టేట్స్\" :op3 \"డాలర్\")))\n",
      "      :ARG2 (a / amr-unknown))\n",
      "\n",
      "# ::id 13965-vi-VN\n",
      "# ::en_utt what is the exchange rate between [currency_name : u. s. d.] and [currency_name : euro]\n",
      "# ::annot_utt tỉ giá hối đoái của [currency_name : đô la mỹ] và [currency_name : đồng euro] là bao nhiêu\n",
      "# ::snt tỉ giá hối đoái của đô la mỹ và đồng euro là bao nhiêu\n",
      "(r / rate-01\n",
      "      :ARG1 (e / exchange-01\n",
      "            :ARG1 (c / currency :name (n / name :op1 \"đồng\" :op2 \"euro\"))\n",
      "            :ARG3 (c2 / currency :name (n2 / name :op1 \"đô\" :op2 \"la\" :op3 \"mỹ\")))\n",
      "      :ARG2 (a / amr-unknown))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(''.join(train[950:980]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c427a-25a5-40f7-8615-887f9b4bbc97",
   "metadata": {},
   "source": [
    "# Step 4: A quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba6987f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean len sent by tokens: 5.37\n",
      "Mean len sent by chars: 32.01\n",
      "\n",
      "Std len sent by tokens: 2.98\n",
      "Std len sent by chars: 16.62\n"
     ]
    }
   ],
   "source": [
    "# All languages (tokenization by splitting on white space)\n",
    "print(f'Mean len sent by tokens: {np.mean(snt_lens_tokens):.2f}')\n",
    "print(f'Mean len sent by chars: {np.mean(snt_lens_chars):.2f}')\n",
    "print()\n",
    "print(f'Std len sent by tokens: {np.std(snt_lens_tokens):.2f}')\n",
    "print(f'Std len sent by chars: {np.std(snt_lens_chars):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72af95c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean len EN sent by tokens: 8.16\n",
      "Mean len EN sent by chars: 47.23\n"
     ]
    }
   ],
   "source": [
    "# English only data\n",
    "print(f'Mean len EN sent by tokens: {np.mean(snt_lens_tokens_en):.2f}')\n",
    "print(f'Mean len EN sent by chars: {np.mean(snt_lens_chars_en):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5caefdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic programming solution to edit distance between entities\n",
    "# from: https://stackoverflow.com/questions/2460177/edit-distance-in-python\n",
    "# want to identify examples which did not modify original English entity\n",
    "# in some cases, the same L1 entity exists but written in L2 (hard to identify)\n",
    "\n",
    "def levenshteinDistance(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a13b5e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = '[person : martin luther king junior]'\n",
    "s2 = '[person : အောင်ဆန်းစုကြည်]'\n",
    "\n",
    "levenshteinDistance(s1, s2)\n",
    "\n",
    "match = re.search(r'\\[(.*)\\]', s2)\n",
    "math.ceil(len(match.group(0))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62430410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60031\n",
      "28076\n"
     ]
    }
   ],
   "source": [
    "# Sentences were translated and annotated by professional annotators, see original Massive paper (FitzGerald et al, 2023)\n",
    "# Annotation in some languages is incomplete.\n",
    "# How many entity annotations actually exist in the original MASSIVE data? \n",
    "# Examples where the English entity/string was not localized?\n",
    "# Need to estimate, looking for strings that are very different from English tokens \n",
    "\n",
    "cnt_entities, cnt_localized_entities = 0, 0\n",
    "\n",
    "for i in range(len(list_amrs)):\n",
    "    \n",
    "    example_utt_1, example_utt_2 = [], []\n",
    "\n",
    "    #rand_example = random.sample(list_amrs, 1)[0]\n",
    "    \n",
    "    rand_example = list_amrs[i]\n",
    "\n",
    "    sentid = rand_example.sentid\n",
    "\n",
    "    all_elements = data[sentid]\n",
    "\n",
    "    for element in all_elements:\n",
    "        en_utt = ''\n",
    "        if element['locale'] == 'en-US':\n",
    "            en_utt = element['annot_utt']  \n",
    "\n",
    "            example_utt_1.append(('en-US', en_utt))\n",
    "            \n",
    "            break \n",
    "\n",
    "    for element in all_elements:\n",
    "\n",
    "        annot_utt = element['annot_utt']\n",
    "\n",
    "        utt = element['utt']\n",
    "\n",
    "        locale = element['locale']\n",
    "\n",
    "        example_utt_1.append((locale, annot_utt))\n",
    "        \n",
    "        if '[' in annot_utt:\n",
    "            cnt_entities+=1\n",
    "            if '[' in annot_utt and '[' in en_utt:\n",
    "                \n",
    "                annot_ent = re.search(r'\\[(.*)\\]', annot_utt).group(0)\n",
    "                en_ent = re.search(r'\\[(.*)\\]', en_utt).group(0)\n",
    "                \n",
    "                if len(en_utt)<len(annot_utt):\n",
    "                    short_utt = en_utt\n",
    "                else:\n",
    "                    short_utt = annot_utt\n",
    "\n",
    "                # arbitrary threshold, ratio of entity length to utterance length\n",
    "                \n",
    "                threshold_len = math.ceil(len(short_utt) * 0.2)\n",
    "                \n",
    "                if levenshteinDistance(annot_ent, en_ent)>threshold_len:\n",
    "                    \n",
    "                    cnt_localized_entities += 1\n",
    "            \n",
    "                  \n",
    "print(cnt_entities)           \n",
    "print(cnt_localized_entities)       \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8178956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_utt_1 = []\n",
    "\n",
    "rand_example = random.sample(list_amrs, 1)[0]\n",
    "\n",
    "sentid = rand_example.sentid\n",
    "\n",
    "all_elements = data[sentid]\n",
    "\n",
    "for element in all_elements:\n",
    "    en_utt = ''\n",
    "    if element['locale'] == 'en-US':\n",
    "        en_utt = element['annot_utt']  \n",
    "\n",
    "        example_utt_1.append(('en-US', en_utt))\n",
    "\n",
    "        break \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4715de4-008b-4e08-8db4-a54758e5b2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('en-US', 'what causes in burmuda triangle')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_utt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af0f0955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are some updates about the stock market\n",
      "definition of velocity\n",
      "please look up exchange between us and mexico\n",
      "can you describe to me what a pineapple looks like\n",
      "what is the dollar against the pound\n",
      "what does potato mean\n",
      "how much is the british pound\n",
      "what is the exchange rate of u. s. d. to cdn\n",
      "how many people live in san francisco\n",
      "please search for this word\n"
     ]
    }
   ],
   "source": [
    "# a few example utterances from the EN split\n",
    "\n",
    "for i in range(len(list_amrs[0:10])):\n",
    "    print(list_amrs[i].sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
